---
title: "Come to the Dark Side"
subtitle: "Text Mining Learning Lab 2"
author: "YOUR NAME HERE"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

Recall from TM Learning Lab 1 that one of our goals for these labs is to produce a simplistic replication of the analyses by Rosenberg et al (2021). Specifically, we want to compare the sentiment of tweets about the [Next Generation Science Standards](https://www.nextgenscience.org) (NGSS) and [Common Core State Standards](http://www.corestandards.org) (CCSS) in order to better understand public reaction to these two curriculum reform efforts.

In TM Learning Lab 3: Come to the Dark Side we focus on the use of lexicons for text analysis and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards.

### 1a. Load Libraries

Before introducing the {vader} package, let's load some two familiar packages that we can also use for performing sentiment analysis:

```{r}
library(tidyverse)
library(tidytext)
library(here)
```

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment Reasoning (VADER), a rule-based model for general sentiment analysis of social media text and specifically attuned to measuring sentiment in microblog-like contexts.

To learn more about the {vader} package and its development, take a look at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious Rule-based Model forSentiment Analysis of Social Media Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the VADER library:

```{r}
library(vader)
```

**Note:** The {vader} package can take quite some time to run on a large dataset like ours so it's included in our R-each section for you to explore at your leisure. The example show in the reach section includes just a small(ish) subset of tweets for demonstration purposes.

### 1b. Import Data

Finally, let's use the by now familiar `read_csv()` and `here()` functions to import our `ss_tidy_tweets.csv` data from Learning Lab 1 that we'll be using for our sentiment analysis:

```{r}
ss_tidy_tweets <- read_csv(here("data", "ss_tidy_tweets.csv"), 
                   col_types = cols(author_id = col_character(),
                                    conversation_id = col_character(), 
                                    id = col_character()))
```

#### [Your Turn]{style="color: green;"} â¤µ

Use the head, tail, glimpse or other view function or method to quickly inspect our data and make sure everything looks in order:

```{r}
glimpse(ss_tidy_tweets)
```

------------------------------------------------------------------------

## 2. WRANGLE

Since we're working tweets that have already been tidied in Learning Lab 1, for this learning lab our data wrangling skills will focus on how to:

a.  **Get Sentiments**. We introduce sentiment lexicons, take a look at how each lexicon assigns sentiment, and learn a little bit more about how each was validated.

b.  **Join Sentiment.** We learn yet another join function, the `inner_join()`, for appending sentiment values to our data frame.

For a quick overview of the different join functions with helpful visuals, here is an additional resource to the one shared in Learning Lab 1: [\<https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\>](https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti){.uri}

### 2a. Get Sentiments

Recall from our readings that sentiment analysis tries to evaluate words for their emotional association. Silge & Robinson point out that, "one way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words." This isn't the only way to approach sentiment analysis, but it is an easier entry point into sentiment analysis and one that often-used.

The {tidytext} package introduced in Learning Lab 1 provides access to several sentiment lexicons based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, etc.

The three general-purpose lexicons we'll focus on are:

-   `AFINN` assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. AFINN is very similar to the lexicon used in our guiding study by Rosenberg et al. (2021)

-   `bing` categorizes words in a binary fashion into positive and negative categories.

-   `nrc`Â categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

**Note that if this is your first time using the AFINN and NRC lexicons and you are working in RStudio Desktop, you'll be prompted to download both.** Respond yes to the prompt by entering "1" and the NRC and AFINN lexicons will download. You'll only have to do this the first time you use the NRC lexicon.

Let's take a quick look at each of these lexicons using the `get_sentiments()` function and assign them to their respective names for later use:

```{r}
afinn <- get_sentiments("afinn")

afinn
```

```{r}
bing <- get_sentiments("bing")

bing
```

```{r}
nrc <- get_sentiments("nrc")

nrc
```

And just out of curiosity, let's take a look at the `loughran` lexicon as well:

```{r}
loughran <- get_sentiments("loughran")

loughran
```

#### [Your Turn]{style="color: green;"} â¤µ

In the space below, answer the following questions:

1.  How were these sentiment lexicons put together and validated? Hint: take a look at [Chapter 2 from Text Mining with R](https://www.tidytextmining.com/sentiment.html#the-sentiments-datasets).

    -   

2.  Why do you think we should be cautious when using and interpreting these lexicons?

    -   

#### Join Sentiments

We've reached the final step in our data wrangling process before we can begin exploring our data to address our research questions.

In the previous section, we usedÂ `anti_join()`Â to remove stop words in our dataset. For sentiment analysis, we're going use the `inner_join()` function to do something similar.

Unlike the `anti_join()` function that removes rows that contain words matching those in our stop words dictionary, `inner_join()` allows us to keep only the rows with words that match words in our sentiment lexicons, or dictionaries, along with the sentiment measure for that word from the sentiment lexicon.

Let's use `inner_join()` to combine our two `tidy_tweets` and `afinn` data frames, keeping only rows with matching data in the `word` column:

```{r}
sentiment_afinn <- inner_join(ss_tidy_tweets, afinn, by = "word")

sentiment_afinn %>% select(word, value)
```

Notice that each word in your `sentiment_afinn` data frame now contains a value ranging from -5 (very negative) to 5 (very positive).

Let's join and take a look at the our sentiments according to bing:

```{r}
sentiment_bing <- inner_join(ss_tidy_tweets, bing, by = "word")

sentiment_bing  %>% select(word, sentiment)
```

#### [Your Turn]{style="color: green;"} â¤µ

1.  What are three words in our our `sentiments_afinn` data frame are the most that are most negative? Positive?

    -   

Use the code chunk below to create a `sentiment_nrc` data frame and answer the following questions:

```{r}
sentiment_nrc <- inner_join(ss_tidy_tweets, nrc, by = "word")

sentiment_nrc  %>% select(word, sentiment)
```

1.  What do you notice about the change in the number of observations (i.e. words) between the `ss_tidy_tweets` and data frames with sentiment values attached? Why did this happen?

    -   

## 3. EXPLORE

Now that we have our tweets tidied and sentiments joined, we're ready for a little data exploration. As highlighted in Learning Labs 1 & 2, calculating summary statistics and data visualization are a key part of exploring text. One goal in this phase is explore questions that drove the original analysis. Topics addressed in Section 3 include:

a.  **Count Sentiments**. Similar to Learning Lab 1, we put together some basic counts of the sentiment assigned to words in our tweets.
b.  **Create Summaries**. In part 2 of Explore, summaries of our sentiment values in order to compare public sentiment between NGSS and CCSS

### 3a. Count Sentiments

Since our primary goals is to compare public sentiment around the NGSS and CCSS state standards, in this section we put together some basic numerical summaries using our familiar tidyverse functions and the lexicons introduce above to see whether tweets are generally more positive or negative for each standard as well as differences between the two.

Let's start with `bing`, our simplest sentiment lexicon, and use the `count` function to count how many times in our `sentiment_bing` data frame "positive" and "negative" occur in `sentiment` column and :

```{r}
summary_bing <- count(sentiment_bing, sentiment, sort = TRUE)

summary_bing
```

Collectively, it looks like our combined dataset has more negative words than positive words.

#### [Your Turn]{style="color: green;"} â¤µ

Since our main goal is to compare positive and negative sentiment between CCSS and NGSS, add the `group_by()` function and supply the appropriate argument again to get `sentiment` summaries for NGSS and CCSS separately:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>%
  count(sentiment) 

summary_bing
```

Looks like CCSS have far more negative words than positive, while NGSS skews much more positive. So far, pretty consistent with the findings in our guiding study!

### 3b. Create Sentiment Summaries

Our last step will be calculate a single sentiment "score" for our tweets that we can use for quick comparison and create a new variable indicating which lexicon we used.

First, let's untidy our data a little by introducing the `spread()` function from the {tidyr} package to transform our `sentiment` column into separate columns for `negative` and `positive` that contains the `n` counts for each:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) 

summary_bing
```

Finally, we'll use the `mutate` function to create two new variables: `sentiment` and `lexicon` so we have a single sentiment score and the lexicon from which it was derived:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(lexicon = "bing") %>%
  relocate(lexicon)

summary_bing
```

There we go, now we can see that CCSS scores negative, while NGSS is overall positive. By quite a bit in fact!!

We're going to calculate a quick score using the `afinn` lexicon now, but first recall that AFINN provides a value from -5 to 5 for each:

```{r}
sentiment_afinn %>% select(word, value)
```

To calculate a summary score, we will need to first group our data by `standards` again and then use the `summarise` function to create a new `sentiment` variable by adding all the positive and negative scores in the `value` column:

```{r}
summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```

Again, CCSS is overwhelmingly negative in terms of total negative words, while NGSS is overall positive!

#### [Your Turn]{style="color: green;"} â¤µ

Now it your turn to calculate a single sentiment score for NGSS and CCSS using the `nrc` lexicon. The `nrc` lexicon, however, contains "positive" and "negative" values just like `bing` and `loughan`, but also includes values like "trust" and "sadness" as shown below. You will need to use the `filter()` function as an additional step to select rows that only contain "positive" and "negative" values.

```{r}
summary_nrc <- sentiment_nrc %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "nrc")  %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative)

summary_nrc
```

1.  Are these findings above still consistent with those from other lexicons above?

    -   

## 4. MODEL

Recall from the PREPARE section that the Rosenberg et al. study was guide by the following questions:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for teachers differ from non-teachers?
3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?
4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?
5.  How does public sentiment vary over time?

Similar to our sentiment summary using the AFINN lexicon, the Rosenberg et al. study used the -5 to 5 sentiment score from the SentiStrength lexicon to answer RQ \#1. To address the remaining questions the authors used a mixed effects model (also known as multi-level or hierarchical linear models via the lme4 package in R.

Collectively, the authors found that:

1.  The SentiStrength scale indicated an overall neutral sentiment for tweets about the Next Generation Science Standards.
2.  Teachers were more positive in their posts than other participants.
3.  Posts including \#NGSSchat that were posted outside of chats were slightly more positive relative to those that did not include the \#NGSSchat hashtag.
4.  The effect upon individuals of being involved in the \#NGSSchat was positive, suggesting that there is an impact on individuals---not tweets---of participating in a community focused on the NGSS.
5.  Posts about the NGSS became substantially more positive over time.

## 5. COMMUNICATE

In this learning lab, we focused on the use of lexicons for text analysis and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards. Below, add a few notes in response to the following prompts:

1.  One thing I took away from this learning lab:

    -   

2.  One thing I want to learn more about:

    -   

Congratulations - you've made it to the end! To complete your work, you can click the drop down arrow at the top of the file, then select "Knit top HTML". This will create a report in your Files pane that serves as a record of your code and its output you can open or share.

## R-each (Optional)

R you ready to Come to the Dark Side?!?! If yes, then try our R-each! For our reach in this lab, you aim is to compute sentiment scores for your own data or the data in this lab using the {vader} package demonstrated below.

![](img/dark-side.jpeg){width="80%"}

As noted in the PERPARE section, the {vader} package is for the Valence Aware Dictionary for sEntiment Reasoning (VADER), a rule-based model for general sentiment analysis of social media text and specifically attuned to measuring sentiment in microblog-like contexts.

Unlike our our previous approach of assigning sentiment to each individual word in our corpus of tweets, the VADER assigns a number of different sentiment measures based on the context of the entire social-media post or in our case a tweet. So rather than working with tidy text, we'll need to work with our tweets in their original format.

VADER can also take a little while to run since it's computationally intensive so let's read in our original `ccss-tweets.csv` and take at just a sample of 500 untidied CCSS tweets using the `sample_n()` function

```{r}
ccss_sample <- read_csv(here("data", "ccss-tweets.csv")) %>%
  sample_n(500)

ccss_sample
```

Note above that we passed our `read_csv()` output directly to our `sample()` function rather than saving a new data frame object, passing that to `sample_n()`, and saving as another data frame object. The power of the `%>%` pipe!

On to the Dark Side. The {vader} package basically has just one function, `vader_df()` that does one thing and expects just one column from one frame. He's very single minded! Let's give VADER our `ccss_sample` data frame and include the `$` operator to include only the `text` column containing our tweets.

```{r}
summary_vader <- vader_df(ccss_sample$text)

summary_vader
```

Take a look at `vader_summary` data frame using the `View()` function and sort by most positive and negative tweets.

Does it generally seem accurately identify positive and negative tweets? Could you find any that you think were mislabeled?

-   YOUR RESPONSE HERE

Hutto, C. & Gilbert, E. (2014) provide an excellent summary of the VADER package on their [GitHub repository](scores) and I've copied and explanation of the scores below:

-   TheÂ `compound`Â score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.

**NOTE:**Â TheÂ `compound`Â score is the one most commonly used for sentiment analysis by most researchers, including the authors.

Let's take a look at the average compound score for our CCSS sample of tweets:

```{r}
mean(summary_vader$compound)
```

Overall, does your CCSS tweets sample lean slightly negative or positive? Is this what you expected?

What if we wanted to compare these results more easily to our other sentiment lexicons just to check if result are fairly consistent?

The author's note that it is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values are:

-   **positive sentiment**:Â `compound`Â score \>= 0.05

-   **neutral sentiment**: (`compound`Â score \> -0.05) and (`compound`Â score \< 0.05)

-   **negative sentiment**:Â `compound`Â score \<= -0.05

Let's give that a try and see how things shake out:

```{r}
vader_results <- summary_vader %>% 
  mutate(sentiment = ifelse(compound >= 0.05, "positive",
                            ifelse(compound <= -0.05, "negative", "neutral"))) %>%
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>% 
  relocate(positive)

vader_results
```

Not quite as bleak as we might have expected according to VADER! But then again, VADER brings an entirely different perspective coming from the dark side.

Comparing the results from those of other sentiment lexicons, why do you think we should cautions both in our selection of lexicons for use, as well as in their interpretation?

-   YOUR RESPONSE HERE

#### [Your Turn]{style="color: green;"} â¤µ

In a separate R script file, try using VADER to perform a sentiment analysis of the NGSS tweets and see how they compare. Post your working code in the chunk below.

```{r}
ngss_sample <- read_csv(here("data", "ngss-tweets.csv")) %>%
  sample_n(500)

summary_vader_ngss <- vader_df(ngss_sample$text)

summary_vader_ngss

vader_results_ngss <- summary_vader_ngss %>% 
  mutate(sentiment = ifelse(compound >= 0.05, "positive",
                            ifelse(compound <= -0.05, "negative", "neutral"))) %>%
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>% 
  relocate(positive)

vader_results_ngss
```

How do your results compare to the CSSS sample of tweets?

-   YOUR RESPONSE HERE
